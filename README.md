# NLP_Datawhale
records of NLP learning process from Datawhale

## Task 1 赛题理解
本次赛题是通过对已进行匿名化处理（中文-->数字）的文本进行分类（分成14类），对应关系如下——{'科技': 0, '股票': 1, '体育': 2, '娱乐': 3, '时政': 4, '社会': 5, '教育': 6, '财经': 7, '家居': 8, '游戏': 9, '房产': 10, '时尚': 11, '彩票': 12, '星座': 13}。

另，训练集和测试集中的原始数据形式如下：
![image](https://github.com/KennyNgZW/NLP_Datawhale/blob/master/dataform_text_classification.png)

赛题目的是希望能了解需要分析的内容中数字之间的关系，可以直接提取他们的关系而不需要其他的特征构造的方式或者简单的统计方式（分组之后进行sum, mean, max, nunique之类）。

## Task 2 EDA
本节通过对文本内容的频数及标签的分布来初步了解目标文本的信息：
* 文本内容：文本长度不均，且长度的较为离散；经过匿名化后，也较难看出其内容的相关性和规律；
* 标签：分布不均，集中在部分的分类，对训练后的模型的准度带来影响。

## Task 3 Feature Engineering
对于不定的长度和不能直接用于模型学习预测的文本数据，首先要做的就是先变成能识别的数字类型数据（integer, float），然后再对其进行特征的提取，也就包含以下的步骤与方法：
1. 将文本转化为数字型数据：
  * 对所有document（即一个text）求集合（set），并编号按顺序从1开始到最后一个字符来编号；
  * （选做）对已经编码好的character进行one-hot encode
2. 提取特征
  * bag of words(词袋): 即Count Vectors，每个文档的字/词（ngram）都可以用其在整个语言库（训练集/测试集）的唯一性（单单这个字/词）的次数来表示。这也是CountVectorizer的原理基础。（仅能用于英文/数字的统计）
  * tf-idf：第一部分是词语频率（Term Frequency），第二部分是逆文档频率（Inverse Document Frequency），然后两个部分相乘。
    * TF(t)= 该词语在当前文档出现的次数 / 当前文档中词语的总数
      * 表示该词在此（类）文档中的重要性
    * IDF(t)= log_e（文档总数 / 出现该词语的文档总数）
      * 表示该类文章是否普及（如果全部文章都有，如“这”“事情”等常用词，则此项为0，因为就等于1），否则就会被放大/缩小（设括号里的值为x，当x<e，0<idf<1; x=e, idf=1; x>e, idf>1——如10篇文章中只有一篇有“SpaceX”，则“SpaceX”的idf值为2左右，整体的值就会被放大）
3. 通过不同的分类器，对提取的特征与标签进行学习、匹配、预测。

## Task 4 FastText
通过简单的神经网络，对长度不定的文本（已经转化为数字类型）进行修剪（统一长度）和降维（高维度映射到低纬度的连接层）。

